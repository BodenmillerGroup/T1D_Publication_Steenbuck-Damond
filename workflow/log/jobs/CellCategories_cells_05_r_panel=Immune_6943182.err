Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 48
Rules claiming more threads will be scaled down.
Select jobs to execute...

[Sun Dec  3 17:50:05 2023]
rule CellCategories_cells_05_r:
    input: /home/nsteen/scripts/T1D_analysis/analysis/05_CellCategories_cells_Immune.Rmd, /home/nsteen/scripts/T1D_analysis/docs/04_QualityControl_cells_Immune.html
    output: /home/nsteen/scripts/T1D_analysis/docs/05_CellCategories_cells_Immune.html
    jobid: 0
    benchmark: benchmark/05_CellCategories_cells_Immune.txt
    reason: Forced execution
    wildcards: panel=Immune
    threads: 48
    resources: mem_mb=350000, disk_mb=1000, tmpdir=/sctmp/nsteen, time=24:00:00

Rscript --vanilla -e 'rmarkdown::render("/home/nsteen/scripts/T1D_analysis/.snakemake/scripts/tmp_547rprc.05_CellCategories_cells_Immune.Rmd", output_file="/home/nsteen/scripts/T1D_analysis/docs/05_CellCategories_cells_Immune.html", quiet=TRUE, knit_root_dir = "/home/nsteen/scripts/T1D_analysis", params = list(rmd="/home/nsteen/scripts/T1D_analysis/.snakemake/scripts/tmp_547rprc.05_CellCategories_cells_Immune.Rmd"))'
Activating singularity image /data/nsteen/singularity/sandbox
 - point 10000 of 190000
 - point 20000 of 190000
 - point 30000 of 190000
 - point 40000 of 190000
 - point 50000 of 190000
 - point 60000 of 190000
 - point 70000 of 190000
 - point 80000 of 190000
 - point 90000 of 190000
 - point 100000 of 190000
 - point 110001 of 190000
 - point 120000 of 190000
 - point 130000 of 190000
 - point 140000 of 190000
 - point 150000 of 190000
 - point 160000 of 190000
 - point 170000 of 190000
 - point 180000 of 190000
 - point 190000 of 190000
 - point 10000 of 190000
 - point 20000 of 190000
 - point 30000 of 190000
 - point 40000 of 190000
 - point 50000 of 190000
 - point 60000 of 190000
 - point 70000 of 190000
 - point 80000 of 190000
 - point 90000 of 190000
 - point 100000 of 190000
 - point 110000 of 190000
 - point 120000 of 190000
 - point 130000 of 190000
 - point 140000 of 190000
 - point 150001 of 190000
 - point 160000 of 190000
 - point 170000 of 190000
 - point 180000 of 190000
 - point 190000 of 190000
 - point 10000 of 190000
 - point 20000 of 190000
 - point 30000 of 190000
 - point 40000 of 190000
 - point 50000 of 190000
 - point 60000 of 190000
 - point 70000 of 190000
 - point 80000 of 190000
 - point 90000 of 190000
 - point 100000 of 190000
 - point 110000 of 190000
 - point 120000 of 190000
 - point 130000 of 190000
 - point 140000 of 190000
 - point 150000 of 190000
 - point 160000 of 190000
 - point 170000 of 190000
 - point 180000 of 190000
 - point 190000 of 190000

Quitting from lines 1170-1215 [cellcats-pheno-scaled] (/sctmp/nsteen/Rtmp5yG0db/tmp_547rprc.05_CellCategories_cells_Immune.Rmd)
Error:
! Recheck cluster attribution
Execution halted
Not cleaning up /home/nsteen/scripts/T1D_analysis/.snakemake/scripts/tmp_547rprc.05_CellCategories_cells_Immune.Rmd
[Mon Dec  4 00:35:06 2023]
Error in rule CellCategories_cells_05_r:
    jobid: 0
    output: /home/nsteen/scripts/T1D_analysis/docs/05_CellCategories_cells_Immune.html

RuleException:
CalledProcessError in line 104 of /home/nsteen/scripts/T1D_analysis/workflow/Snakefile:
Command ' singularity  exec --home /home/nsteen/scripts/T1D_analysis --bind /scratch/nsteen/processing:/home/processing,/scratch/nsteen/T1D_preprocessing:/scratch/nsteen/T1D_preprocessing,/data/nsteen:/data/nsteen /data/nsteen/singularity/sandbox bash -c 'set -euo pipefail;  Rscript --vanilla -e '\''rmarkdown::render("/home/nsteen/scripts/T1D_analysis/.snakemake/scripts/tmp_547rprc.05_CellCategories_cells_Immune.Rmd", output_file="/home/nsteen/scripts/T1D_analysis/docs/05_CellCategories_cells_Immune.html", quiet=TRUE, knit_root_dir = "/home/nsteen/scripts/T1D_analysis", params = list(rmd="/home/nsteen/scripts/T1D_analysis/.snakemake/scripts/tmp_547rprc.05_CellCategories_cells_Immune.Rmd"))'\'''' returned non-zero exit status 1.
  File "/home/nsteen/scripts/T1D_analysis/workflow/Snakefile", line 104, in __rule_CellCategories_cells_05_r
  File "/home/nsteen/data/conda/envs/snakemake_cluster/lib/python3.10/concurrent/futures/thread.py", line 58, in run
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
